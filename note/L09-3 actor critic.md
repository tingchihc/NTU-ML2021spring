## yt  
  * https://www.youtube.com/watch?v=kk6DqWreLeU  

## Actor-critic  
  * Critic: Given actor ğœƒ, how good it is when observing ğ‘  (and taking action ğ‘)  
  * Value function ğ‘‰ğœƒ(ğ‘ ) : When using actor ğœƒ, the discounted cumulated reward expects to be obtained after seeing s  
  * The output values of a critic depend on the actor evaluated.  

## Estimate ğ‘‰ğœƒ(ğ‘ )  
## Monte-Carlo(MC) based approach  
  * The critic watches actor ğœƒ to interact with the environment.  
  * After seeing ğ‘ ğ‘, Until the end of the episode, the cumulated reward is ğºğ‘â€²  
  * After seeing ğ‘ ğ‘, Until the end of the episode, the cumulated reward is ğºb'  

## Temporal-difference(TD) approach  
  * {St,at,rt,St+1}...  
  * ğ‘‰ğœƒ(ğ‘ ğ‘¡) = ğ‘Ÿğ‘¡ + ğ›¾ğ‘Ÿğ‘¡+1 + ğ›¾2ğ‘Ÿğ‘¡+2 â€¦  
  * ğ‘‰ğœƒ(ğ‘ ğ‘¡+1) = ğ‘Ÿğ‘¡+1 + ğ›¾ğ‘Ÿğ‘¡+2 â€¦  
  * ğ‘‰ğœƒ(ğ‘ ğ‘¡+1) = ğ›¾ğ‘‰ğœƒ(ğ‘ ğ‘¡)+rt  

