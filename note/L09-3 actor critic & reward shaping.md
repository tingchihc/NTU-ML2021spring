## yt  
  * https://www.youtube.com/watch?v=kk6DqWreLeU  
  * https://www.youtube.com/watch?v=73YyF1gmIus  

## Actor-critic  
  * Critic: Given actor ðœƒ, how good it is when observing ð‘  (and taking action ð‘Ž)  
  * Value function ð‘‰ðœƒ(ð‘ ) : When using actor ðœƒ, the discounted cumulated reward expects to be obtained after seeing s  
  * The output values of a critic depend on the actor evaluated.  

## Estimate ð‘‰ðœƒ(ð‘ )  
## Monte-Carlo(MC) based approach  
  * The critic watches actor ðœƒ to interact with the environment.  
  * After seeing ð‘ ð‘Ž, Until the end of the episode, the cumulated reward is ðºð‘Žâ€²  
  * After seeing ð‘ ð‘, Until the end of the episode, the cumulated reward is ðºb'  

## Temporal-difference(TD) approach  
  * {St,at,rt,St+1}...  
  * ð‘‰ðœƒ(ð‘ ð‘¡) = ð‘Ÿð‘¡ + ð›¾ð‘Ÿð‘¡+1 + ð›¾2ð‘Ÿð‘¡+2 â€¦  
  * ð‘‰ðœƒ(ð‘ ð‘¡+1) = ð‘Ÿð‘¡+1 + ð›¾ð‘Ÿð‘¡+2 â€¦  
  * ð‘‰ðœƒ(ð‘ ð‘¡+1) = ð›¾ð‘‰ðœƒ(ð‘ ð‘¡)+rt  

![Image of Yaktocat](https://github.com/ting-chih/NTU-ML2021spring/blob/main/image/version3.5.png)  
![Image of Yaktocat](https://github.com/ting-chih/NTU-ML2021spring/blob/main/image/version4.png)  
![Image of Yaktocat](https://github.com/ting-chih/NTU-ML2021spring/blob/main/image/tipofactor-critic.png)  

## Reward shaping -> The developers define extra rewards to guide agents  
  * If ð‘Ÿð‘¡ = 0 in most cases -> We donâ€™t know actions are good or bad.  
